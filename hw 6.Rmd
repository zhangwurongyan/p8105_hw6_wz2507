---
title: "hw6"
author: "Wurongyan Zhang"
date: "11/20/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(modelr)
library(purrr)
```

# Problem 1

## data cleaning
```{r,warning=FALSE,message=FALSE}
birthweight <- read_csv("data/birthweight.csv") %>% mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace)) %>% janitor::clean_names()

summary(is.na(birthweight)) %>% knitr::kable()
```

From the summary above, we can see that there are no missing data. I converted babysex, frace, malform and mrace to factor. The data set contains 4342 observations with in total 20 variables.

## regression model

```{r}
model1 = lm(bwt~., data = birthweight)
step(model1, direction = "both")
```

The model chosen from stepwise regression is:
```{r}
swmodel=lm(bwt~babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight)
summary(swmodel)
```

The plot of model resuduals against fitted values are shown below:
```{r}
birthweight %>% 
  add_predictions(swmodel) %>% 
  add_residuals(swmodel) %>% ggplot(aes(x=pred, y=resid))+
  geom_point(color="pink") + geom_line(aes(y=0))+labs(title = "residuals against fitted values", x="prediction", y="residuals")
```

## comparing models

In order to compare those three models, I would like to use cross validation.

The three models are :
```{r}

model_main = lm(bwt ~ gaweeks ,data = birthweight)

model_head = lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex ,data = birthweight)

```

$$stepwise: bwt\sim babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken\\ main :bwt \sim gaweeks \\head:bwt \sim bhead + blength + babysex + bhead * blength * babysex$$

```{r}
set.seed(100)

cv_bw = birthweight %>% crossv_mc(100) %>% mutate(train = map(train, as_tibble), test = map(test, as_tibble))

cv_bw = cv_bw %>% mutate(
  stepwise = map(train, ~lm(bwt ~babysex + bhead + blength +delwt +fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .)),
  main = map(train, ~lm(bwt ~ gaweeks, data=.)),
  head = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength * babysex ,data =.))
) %>% mutate(rmse_stepwise = map2_dbl(stepwise, test, ~rmse(model = .x, data = .y)),
             rmse_main = map2_dbl(main, test, ~rmse(model = .x, data = .y)),
             rmse_head= map2_dbl(head, test, ~rmse(model = .x, data = .y)))

cv_bw %>% select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, color=model)) + geom_violin() + labs(title = "comparison of the models")
```

From the violin plot, we can see that the model from stepwise selection has the lowest rmse among those three models so the this model has better fit compare to the other two. For the comparison between main effect model and the interaction model, the interaction model has lower rmse than the other one.

## Problem 2

```{r, message=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

# bootstrap

```{r,message=FALSE}
set.seed(100)

results = weather_df %>% 
  bootstrap(5000) %>% mutate(model = map(strap, ~lm(tmax~tmin, data = .x)),coefficient = map(model, broom::tidy),  summary = map(model, broom::glance)) %>% 
  select(-strap,-model) %>% unnest(summary, coefficient) %>% mutate(term=recode(term, "(Intercept)" = "beta0", "tmin"="beta1")) %>%
  select(r.squared, adj.r.squared, term, estimate)  %>%  pivot_wider(names_from = term, values_from = estimate) %>% mutate(log = log(beta0*beta1))

results[1:5,] %>% 
  select(r.squared, adj.r.squared,log) %>% knitr::kable()
```

We can see from the glance of the data set that the data contains our main interest of $\hatR^2$ and $log(\hat\beta_0 * \hat\beta_1)$.



## distribution of log beta

```{r}
results %>% ggplot(aes(x = log)) + geom_density() +labs(title = "distribution of log(beta0*beta1)",x = "log(beta0*beta1)")

```

```{r}
quantile(pull(results,log),c(0.25,0.75))
```

The 95% confidence interval for $log(\hat\beta_0 * \hat\beta_1)$ is between (1.997, 2.029). From the plot we can see that the distribution is kind of normal with a little longer tail on the left side.


## distribution of r-squared

```{r}
results %>% ggplot(aes(x = r.squared)) + geom_density() +labs(title = "distribution of r squared",x = "r squared")

```

```{r}
quantile(pull(results,r.squared),c(0.25,0.75))
```

The 95% confidence interval for $R^2$ is between (0.906, 0.917). From the plot we can see that the distribution is kind of normal and a little bit right skewed.

















